# -*- coding: utf-8 -*-
"""Untitled4.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lsu-cQXImXObc-Tf9RQeuJ3cVjWJM9HW
"""

!pip install pyarrow==19.0.0

!pip install -U transformers datasets accelerate bitsandbytes peft gradio

import torch
from transformers import AutoTokenizer, AutoModelForCausalLM

MODEL_ID = "ibm-granite/granite-3.3-2b-instruct"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32,
    device_map="auto"
)

from datasets import Dataset

data = [
    {"instruction": "How can I build a monthly budget on a salary of 25,000 INR?", "response": "Start by categorizing fixed expenses, variable expenses, and set a savings goal. Allocate at least 20% of your income to savings and investments."},
    {"instruction": "List top three tax saving options for Indian students.", "response": "Consider ELSS mutual funds, PPF accounts, and tuition fee receipts under Section 80C."},
    {"instruction": "How do I track my daily spending?", "response": "You can use personal finance apps, or maintain a daily log in a spreadsheet to monitor all expenses."},
    # Add more diverse Q&A pairs for better training
]

dataset = Dataset.from_list(data)

def format_chat(example):
    messages = [
        {"role": "system", "content": "You are a friendly and knowledgeable personal finance assistant for India. Give explanations in simple language and encourage good habits."},
        {"role": "user", "content": example["instruction"]},
        {"role": "assistant", "content": example["response"]},
    ]
    return {"text": tokenizer.apply_chat_template(messages, tokenize=False)}

dataset = dataset.map(format_chat)

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=256)

tokenized_dataset = dataset.map(tokenize_function, batched=True, batch_size=4)
split_dataset = tokenized_dataset.train_test_split(test_size=0.2)

from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

MODEL_ID = "ibm-granite/granite-3.3-2b-instruct"
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)
model = AutoModelForCausalLM.from_pretrained(
    MODEL_ID,
    torch_dtype=torch.float16 if DEVICE == "cuda" else torch.float32
)
model.to(DEVICE)

# Tokenize your dataset (example)
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=256)

tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Split dataset into train and test
split_dataset = tokenized_dataset.train_test_split(test_size=0.2)

# Then define training arguments and trainer
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./personal_finance_chatbot",
    learning_rate=3e-5,
    per_device_train_batch_size=2,
    per_device_eval_batch_size=2,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="./logs",
    logging_steps=5,
    save_total_limit=2,
    report_to="none"
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=split_dataset["train"],
    eval_dataset=split_dataset["test"]
)

trainer.train()

import gradio as gr

def predict(user_message, chat_history):
    messages = [{"role": "system", "content": "You are a helpful personal finance assistant specializing in Indian financial advice."}]
    for human, ai in chat_history:
        messages.append({"role": "user", "content": human})
        messages.append({"role": "assistant", "content": ai})
    messages.append({"role": "user", "content": user_message})

    inputs = tokenizer.apply_chat_template(messages, tokenize=True, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=128, temperature=0.7)
    response = tokenizer.decode(outputs[0][inputs["input_ids"].shape[-1]:], skip_special_tokens=True)
    return response

def gradio_respond(message, history):
    response = predict(message, history)
    history.append((message, response))
    return history, history

with gr.Blocks() as demo:
    gr.Markdown("## ðŸ¤– Personal Finance Chatbot (India)")
    chatbot = gr.Chatbot(label="FinanceBot", show_copy_button=True, height=350)
    user_msg = gr.Textbox(show_label=False, placeholder="Ask about tax, savings, budgeting...")
    clear_btn = gr.Button("Clear Conversation")
    examples = gr.Examples(
        examples=[
            "How to save more on taxes in India?",
            "Tips for first salary investment",
            "Best ways to manage monthly budget"
        ],
        inputs=user_msg
    )
    user_msg.submit(gradio_respond, [user_msg, chatbot], [chatbot, chatbot])
    clear_btn.click(lambda: ([], []), None, [chatbot, chatbot])

demo.launch(share=True)